{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Data Analysis of the Data Science Job Posting On GlassDoor**","metadata":{}},{"cell_type":"markdown","source":"## **Table of Contents**\n1. [Introduction](#Introduction)\n    * [Project Background](#Project-Background)\n    * [Primary Objective](#Primary-Objective)\n    * [Data Set](#Data-Set)\n    * [Methodology](#Methodology)\n2. [Data Preparation and Cleaning](#data-preparation)\n    * [Importation](#importation)\n    * [Understand the Data](#data-overview)\n    * [Data Cleaning](#data-cleaning)\n3. [Exploratory Data Analysis](#EDA)\n4. [Conclusion](#conclusion)\n5. [Discussion](#discussion)\n6. [Acknowledgement](#acknowledgement)\n","metadata":{}},{"cell_type":"markdown","source":"<a id='Introduction'></a>\n## **Introduction**","metadata":{}},{"cell_type":"markdown","source":"<a id='Project-Background'></a>\n### **Project Background:**\nThe Data Science field has experienced tremendous growth over the past decade. Companies across various industries, including technology, finance, healthcare, and retail, are increasingly leveraging data to make informed decisions. As a result, Data Professionals, who can turn raw data into actionable insights, have become one of the most in-demand professionals globally.\n\nWithin this expansive field, a myriad of titles and positions exist, reflecting a spectrum of responsibilities. Examples include Data Analysts, Data Scientists, Data Engineers, and Machine Learning Scientists. While this project does not intend to clarify the precise definitions of these roles, it aims to provide a comprehensive snapshot of the current Data Science job market, as seen through the job postings on Glassdoor. \n\nAdditionally, I hope that the findings of this project will not only assist me but also aid other job seekers in uncovering valuable insights for navigating the data science job market. ","metadata":{}},{"cell_type":"markdown","source":"<a id='Primary-Objective'></a>\n### **Primary Objective:** ##\nThis project analyzes a dataset of Data Science job postings from Glassdoor to uncover key insights into the job market. The analysis uses various visualizations to explore aspects such as salary estimates, company sizes, and geographical distribution of job postings.","metadata":{}},{"cell_type":"markdown","source":"<a id='Data-Set'></a>\n### **Data Set:**\nThe dataset contains 672 observations and 14 variables, each observation represents a unique job posting for a Data Science-related position on Glassdoor, a popular job search website. The variables provide detailed information about each job posting.\n\nLink: https://www.kaggle.com/datasets/rashikrahmanpritom/data-science-job-posting-on-glassdoor","metadata":{}},{"cell_type":"markdown","source":"<a id='Methodology'></a>\n### **Methodology:**\n#### 1. Data Cleaning:\n* Refine the 'Job Title' column, consolidating redundant entries into 8 standard, commonly used titles.\n* Split the 'Salary Estimate' column into 'Salary Lower Bound', 'Salary Upper Bound', and 'Salary Midpoint'.\n* Transform the 'Location' column into separate 'Country', 'State', and 'City' columns, standardizing states with their abbreviations.\n* Perform essential cleaning procedures on other columns, tailored to their respective values and data types.\n\n#### 2. Exploratory Data Analysis (EDA):\n* Show the distribution of key variables, such as job titles, company sizes, and business sectors.\n* Visualize the geographical distribution of job postings across the United States and their average salary midpoints.\n* Utilize various graphs to explore the relationships between average salary midpoints and other company characteristics.","metadata":{}},{"cell_type":"markdown","source":"<a id='data-preparation'></a>\n## **Data Preparation and Cleaning:**","metadata":{}},{"cell_type":"markdown","source":"<a id='importation'></a>\n### Importation:","metadata":{}},{"cell_type":"code","source":"# import packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport fuzzywuzzy\nfrom fuzzywuzzy import process\nimport re\nimport geopandas as gpd\nimport plotly.express as px\n\n\n# some display settings:\npd.set_option('display.max_columns', 100)\n\n\n# load the data\ndata = pd.read_csv(\"/kaggle/input/data-science-job-posting-on-glassdoor/Uncleaned_DS_jobs.csv\", index_col='index')","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:13.837963Z","iopub.execute_input":"2023-09-02T22:58:13.838400Z","iopub.status.idle":"2023-09-02T22:58:13.900039Z","shell.execute_reply.started":"2023-09-02T22:58:13.838362Z","shell.execute_reply":"2023-09-02T22:58:13.898963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='data-overview'></a>\n### Understand the data:","metadata":{}},{"cell_type":"code","source":"data.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:13.902436Z","iopub.execute_input":"2023-09-02T22:58:13.902918Z","iopub.status.idle":"2023-09-02T22:58:13.931560Z","shell.execute_reply.started":"2023-09-02T22:58:13.902876Z","shell.execute_reply":"2023-09-02T22:58:13.930339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:13.933259Z","iopub.execute_input":"2023-09-02T22:58:13.933657Z","iopub.status.idle":"2023-09-02T22:58:13.946045Z","shell.execute_reply.started":"2023-09-02T22:58:13.933625Z","shell.execute_reply":"2023-09-02T22:58:13.944935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:13.948314Z","iopub.execute_input":"2023-09-02T22:58:13.948678Z","iopub.status.idle":"2023-09-02T22:58:13.962407Z","shell.execute_reply.started":"2023-09-02T22:58:13.948649Z","shell.execute_reply":"2023-09-02T22:58:13.961530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe(include = 'all')","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:13.965748Z","iopub.execute_input":"2023-09-02T22:58:13.966332Z","iopub.status.idle":"2023-09-02T22:58:14.030376Z","shell.execute_reply.started":"2023-09-02T22:58:13.966302Z","shell.execute_reply":"2023-09-02T22:58:14.029276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check data index uniqueness\ndata.index.duplicated(keep = 'first').sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.031696Z","iopub.execute_input":"2023-09-02T22:58:14.032165Z","iopub.status.idle":"2023-09-02T22:58:14.040624Z","shell.execute_reply.started":"2023-09-02T22:58:14.032126Z","shell.execute_reply":"2023-09-02T22:58:14.039479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the number of missing values in each column\ndata.isna().sum(axis = 0)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.042299Z","iopub.execute_input":"2023-09-02T22:58:14.042785Z","iopub.status.idle":"2023-09-02T22:58:14.061943Z","shell.execute_reply.started":"2023-09-02T22:58:14.042746Z","shell.execute_reply":"2023-09-02T22:58:14.060500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have checked that the data has a non-duplicate index list, and there are no missing values in the data set. Now, let's dive into each column and do some data cleaning.","metadata":{}},{"cell_type":"markdown","source":"<a id='data-cleaning'></a>\n### Data cleaning:","metadata":{}},{"cell_type":"markdown","source":"##### 1. Job Title Cloumn","metadata":{}},{"cell_type":"code","source":"# record the original title just in case\ndata['old title'] = data['Job Title']\n\n# create a coloum to record whether a title has been integrated\ndata['integrated title'] = pd.Series(np.zeros(len(data)), dtype = int)\n\n# remove whitespace, change into lower cases\ndata['Job Title'] = data['Job Title'].str.strip().str.lower()\n\n# check unique values of the titles and their counts\ntitle_counts = data['Job Title'].value_counts()\nprint(title_counts)\n\n# get the count of different occurance frequencies\ntitle_occurance_freq = title_counts.value_counts().sort_index().to_frame()\ntitle_occurance_freq.index.name = 'occurance frequency'\ntitle_occurance_freq.columns = ['count']\nprint(title_occurance_freq, '\\n'*2)\n\nprint('Titles that have an occurance of at least 5 times cover ', \n      (title_counts[title_counts >= 5].sum())/data.shape[0]*100,\n      '% of the raw data')","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.063597Z","iopub.execute_input":"2023-09-02T22:58:14.064163Z","iopub.status.idle":"2023-09-02T22:58:14.086946Z","shell.execute_reply.started":"2023-09-02T22:58:14.064124Z","shell.execute_reply":"2023-09-02T22:58:14.085561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The raw dataset contains 172 unique titles; however, 96 of these titles appear only once, 55 appear twice, and titles occurring at least five times comprise approximately 62% of our raw data. To ensure sufficient data for each title we analyze, we will retain only those titles that occur five times or more, and we will consolidate the remaining titles into these more frequently occurring categories.\n\nTo accomplish this, we will first assess the similarities between the titles that appear less than five times and those that appear at least five times. Based on this assessment, we will then group similar titles under a common title.","metadata":{}},{"cell_type":"code","source":"# Now we use the extract function from fuzzywuzzy to assess the title\n# similarity, it allows you to input a title, and the algorithm will\n# find the top n most similar titles each with a similar score.\ntitles_show_up_5 = title_counts[title_counts >= 5].index.to_list()\n\nfor i in titles_show_up_5:\n    print('These are the similar titles to ', i, '\\n',\n          fuzzywuzzy.process.extract(i, data['Job Title'].unique(), \n                                     limit = 20,\n                                     scorer = fuzzywuzzy.fuzz.token_sort_ratio),\n          '\\n'*2)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.089036Z","iopub.execute_input":"2023-09-02T22:58:14.089452Z","iopub.status.idle":"2023-09-02T22:58:14.143785Z","shell.execute_reply.started":"2023-09-02T22:58:14.089418Z","shell.execute_reply":"2023-09-02T22:58:14.142486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that most similar titles differ primarily in their wording or formatting, such as 'Sr. Data Analyst', 'Senior Data Analyst', and 'Experienced Data Analyst'.\n\nTo streamline these variations, we propose integrating titles that appear fewer than 5 times using a series of conditional checks. Furthermore, we've identified several management-level titles that don't align with the most frequent titles. To address this, we plan to create a new category, 'Data Science Manager', to encompass all manager-level titles.","metadata":{}},{"cell_type":"code","source":"# Start to integrate the titles that appear less than 5 times, which\n# means we will only keep the titles below and integrate others into them.\ntitles_we_keep = titles_show_up_5.copy()\ntitles_we_keep.append('data science manager')\ntitles_we_keep","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.145837Z","iopub.execute_input":"2023-09-02T22:58:14.146283Z","iopub.status.idle":"2023-09-02T22:58:14.153416Z","shell.execute_reply.started":"2023-09-02T22:58:14.146243Z","shell.execute_reply":"2023-09-02T22:58:14.152626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we use a series of if-else check to consolidate the titles. To do that\n# we start by naming some key words to identify certain titles.\n\nsenior = ['senior', 'sr', 'experienced', 'ii', 'iii', 'staff']\nmanager = ['manager', 'management', 'lead', 'principal', 'director',\n           'president', 'vp']\n\nfor i in range(0, len(data)):\n    if data.loc[i, 'Job Title'] not in titles_we_keep:\n        data.loc[i, 'integrated title'] = 1\n        title = data.loc[i, 'Job Title']\n        \n        if any(key in title for key in manager):\n            data.loc[i, 'Job Title'] = 'data science manager'\n        elif 'machine learning' in title:\n            data.loc[i, 'Job Title'] = 'machine learning engineer'\n        elif any(key in title for key in senior):\n            if 'engineer' in title:\n                data.loc[i, 'Job Title'] = 'senior data engineer'\n            elif 'analyst' in title:\n                data.loc[i, 'Job Title'] = 'senior data analyst'\n            elif 'scientist' in title:\n                data.loc[i, 'Job Title'] = 'senior data scientist'\n        elif 'engineer' in title:\n            data.loc[i, 'Job Title'] = 'data engineer'\n        elif 'analyst' in title:\n            data.loc[i, 'Job Title'] = 'data analyst'\n        elif 'scientist' in title:\n            data.loc[i, 'Job Title'] = 'data scientist'","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.154763Z","iopub.execute_input":"2023-09-02T22:58:14.155848Z","iopub.status.idle":"2023-09-02T22:58:14.262260Z","shell.execute_reply.started":"2023-09-02T22:58:14.155816Z","shell.execute_reply":"2023-09-02T22:58:14.260841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are still some titles with unique wordings that are not integrated, \n# we can integrate using the following conditional checks. \ntitles_need_change_manually = data['Job Title'].value_counts().index.to_list()[8:]\n\nfor i in range(0, len(data)):\n    if data.loc[i, 'Job Title'] in titles_need_change_manually:\n        data.loc[i, 'integrated title'] = 1\n        if ('environmental data science' in title or\n           'it partner digital health technology and data science' in title):\n            data.loc[i, 'Job Title']  = 'data analyst'\n        else:\n            data.loc[i, 'Job Title']  = 'data scientist'\n\n# see results\ndata['Job Title'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.263866Z","iopub.execute_input":"2023-09-02T22:58:14.264250Z","iopub.status.idle":"2023-09-02T22:58:14.294351Z","shell.execute_reply.started":"2023-09-02T22:58:14.264217Z","shell.execute_reply":"2023-09-02T22:58:14.293027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that the conditional checks have effectively managed most of the titles that appear only once or twice, and we further refined the integration manually for the unusual cases. Ultimately, we consolidated all the titles into the 8 common categories we have chosen. For those who wish to validate the process, the following code provides a comparison between the original titles and the integrated ones.","metadata":{}},{"cell_type":"code","source":"#pd.set_option('display.max_rows', None)\n#data.loc[data['integrated title'] == 1,  ['Job Title', 'old title']]","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.296024Z","iopub.execute_input":"2023-09-02T22:58:14.296399Z","iopub.status.idle":"2023-09-02T22:58:14.302179Z","shell.execute_reply.started":"2023-09-02T22:58:14.296366Z","shell.execute_reply":"2023-09-02T22:58:14.300655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 2. Salary Estimate column","metadata":{}},{"cell_type":"markdown","source":"The \"salary estimate\" column is not numerical, but consists of strings that contain upper and lower limit estimates provided by Glassdoor or employers. For subsequent analysis, it is necessary to extract these lower and upper limits and convert them into integers. Before proceeding with this, we also want to examine the format of this column to ensure there are no unexpected values.","metadata":{}},{"cell_type":"code","source":"# extract the column\nsalary = data['Salary Estimate']\nprint(salary.head(5), '\\n'*2)\n\n# there are 20 observations (around 3%) do not contain 'Glassdoor est', \n# instead, they contain 'Employer est.'\nprint(salary[~salary.str.contains('(Glassdoor est.)', regex = False)])\n\n# check that all values have the unit of K/thousand\nsalary[~salary.str.contains('K')].shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.309001Z","iopub.execute_input":"2023-09-02T22:58:14.309533Z","iopub.status.idle":"2023-09-02T22:58:14.330419Z","shell.execute_reply.started":"2023-09-02T22:58:14.309499Z","shell.execute_reply":"2023-09-02T22:58:14.328750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we want to create three new columns, salary estimation's upper limit, \n# lower limit, and midpoint. Also remove non-numerical characters\nsalary_info = (data['Salary Estimate'].str.replace(r'[^\\d-]', '', regex=True)\n              .str.split('-'))\ndata['salary_lower'] = [int(value[0]) for value in salary_info] \n#just practice another way to extract the values\ndata['salary_upper'] = list(map(lambda x:int(x[1]), salary_info))\ndata['salary_midpoint'] = (data.salary_lower.astype('int') + \n                           data.salary_upper.astype('int')) / 2","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.334737Z","iopub.execute_input":"2023-09-02T22:58:14.335179Z","iopub.status.idle":"2023-09-02T22:58:14.355420Z","shell.execute_reply.started":"2023-09-02T22:58:14.335149Z","shell.execute_reply":"2023-09-02T22:58:14.354272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3. Rating column","metadata":{}},{"cell_type":"markdown","source":"One thing needs attention for the 'Rating' column is that there are 50 negative values. It is possible that they are actualy missing values. In order to confirm my thought, I went to Glassdoor.com and searched some companies that have a -1 rating in this data set, and indeed all of them\ndo not have a rating because few number of reviews. So we can\nreplace all the -1 in the Rating column to NaN.","metadata":{}},{"cell_type":"code","source":"print(data.Rating.describe())\nprint('There are', sum(data.Rating == -1), '\"-1\" in the Rating column')\ndata.loc[data.Rating == -1, 'Rating'] = np.nan","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.357170Z","iopub.execute_input":"2023-09-02T22:58:14.357807Z","iopub.status.idle":"2023-09-02T22:58:14.375033Z","shell.execute_reply.started":"2023-09-02T22:58:14.357769Z","shell.execute_reply":"2023-09-02T22:58:14.373756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 4. Company Name column","metadata":{}},{"cell_type":"markdown","source":"For the company name column, the biggest problem we can notice is that\neach name comes with its rating, which is already recorded in another\ncolumn. So, we need to get rid of the unnessary postfix.","metadata":{}},{"cell_type":"code","source":"data['Company Name'] = data['Company Name'].str.split('\\n').str[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.376290Z","iopub.execute_input":"2023-09-02T22:58:14.376630Z","iopub.status.idle":"2023-09-02T22:58:14.391765Z","shell.execute_reply.started":"2023-09-02T22:58:14.376602Z","shell.execute_reply":"2023-09-02T22:58:14.390600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5. Location column","metadata":{}},{"cell_type":"markdown","source":"The location of each job posting is consolidated into a single cell in the 'Location' column. To facilitate our future analysis, I will separate this information into 'City', 'State', and 'Country' columns. For the data consistency, I will use the state abbreviations in our newly created 'state' column. Additionally, I have observed that there are some exceptional values, such as 'remote' or location entries that do not conform to the primary wording format.","metadata":{}},{"cell_type":"code","source":"# create lists to identify the states and country name\nUS = ['United States', 'USA', 'united states', 'U.S.']\nDC = ['Washington, D.C.', 'Washington, DC']\n\n# Map of full state names to their abbreviations\nstate_name_to_abbreviation = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO',\n    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH',\n    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI',\n    'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\n# create a list that contains both full name and abbreviations\nstates = list(state_name_to_abbreviation.keys())\nstate_abbrs = list(state_name_to_abbreviation.values())\nstates.extend(state_abbrs)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.392897Z","iopub.execute_input":"2023-09-02T22:58:14.393774Z","iopub.status.idle":"2023-09-02T22:58:14.406677Z","shell.execute_reply.started":"2023-09-02T22:58:14.393730Z","shell.execute_reply":"2023-09-02T22:58:14.405187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the location into seperate city, state/province, and country column\ndata = data.assign(city = '', state = '', country = '')\nlocation_splitted = data.Location.str.strip().str.split(', ')\n\n# some entries in Location column only have state info, while some have \n# city, state, and country. So, we need to check the number of element\n# in the original location column and then assign the values into our\n# new columns\nfor i in range(len(location_splitted)):\n    location  = location_splitted[i]\n    number_of_element = len(location)\n    if data.Location[i] in DC:\n        data.iloc[i, -3:] = ['DC', 'DC', 'United States']\n    elif number_of_element == 1:\n        if location[0] in ['remote', 'Remote']:\n            data.iloc[i, -3:] = ['Remote'] * 3\n        elif location[0] in US:\n            data.iloc[i, -3:] = ['Unknown', 'Unknown', 'United States']\n        elif location[0] in states:\n            data.iloc[i, -3:] = ['Unknown', location[0], 'United States']\n        else:\n            data.iloc[i, -3:] = [np.nan] * 3\n    elif number_of_element == 2:\n        if location[1] in states:\n            data.iloc[i, -3:] = [location[0], location[1], 'United States']\n        elif location[0] in states and location[1] in US:\n            data.iloc[i, -3:] = ['Unknown', location[0], 'United States']\n        else:\n            data.iloc[i, -3:] = [np.nan] * 3\n    elif number_of_element == 3:\n        data.iloc[i, -3:] = [location[0], location[1], location[2]]\n    else:\n        data.iloc[i, -3:] = [np.nan] * 3\n        \n\n# Then we replace the state full names with their abbreviations.\ndata['state'] = data['state'].replace(state_name_to_abbreviation)\n# Specific replacements, it is a city in MD\ndata['state'] = data['state'].replace({'Anne Arundel': 'MD'})  ","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.408369Z","iopub.execute_input":"2023-09-02T22:58:14.409173Z","iopub.status.idle":"2023-09-02T22:58:14.603771Z","shell.execute_reply.started":"2023-09-02T22:58:14.409129Z","shell.execute_reply":"2023-09-02T22:58:14.602579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 6. Size column","metadata":{}},{"cell_type":"markdown","source":"After examining the unique values in the 'Size' column, I noticed that there are '-1' values. Upon further investigation into the companies that are listed with a size of '-1', I observed that their location, founding date, and other information are also missing. Therefore, it is reasonable to replace the '-1' values in the 'Size' column with 'Unknown'.","metadata":{}},{"cell_type":"code","source":"data.loc[data.Size == '-1', 'Size'] = 'Unknown'\ndata.Size.value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.605324Z","iopub.execute_input":"2023-09-02T22:58:14.605879Z","iopub.status.idle":"2023-09-02T22:58:14.616200Z","shell.execute_reply.started":"2023-09-02T22:58:14.605846Z","shell.execute_reply":"2023-09-02T22:58:14.614970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 7. Founded column","metadata":{}},{"cell_type":"code","source":"# Founded column also has -1 value, change them to 'Unknown'\ndata.loc[data['Founded'] == -1, 'Founded'] = 'Unknown'\ndata.Founded.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.618523Z","iopub.execute_input":"2023-09-02T22:58:14.622410Z","iopub.status.idle":"2023-09-02T22:58:14.634790Z","shell.execute_reply.started":"2023-09-02T22:58:14.622368Z","shell.execute_reply":"2023-09-02T22:58:14.633658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 8. Type of ownership column","metadata":{}},{"cell_type":"markdown","source":"For the 'Ownership' column, We can combine the similar types while transfering -1 to Unknown.","metadata":{}},{"cell_type":"code","source":"public_ownership = ['Government']\nprivate_ownership = ['Subsidiary or Business Segment', 'Private Practice / Firm']\nnon_profit = ['College / University', 'Hospital']\nother_ownership = ['Self-employed', 'Contract']\n\nfor index, row in data.iterrows():\n    ownership = row['Type of ownership']\n    if ownership == '-1':\n        data.loc[index, 'Type of ownership'] = 'Unknown'\n    elif ownership in public_ownership:\n        data.loc[index, 'Type of ownership'] = 'Company - Public'\n    elif ownership in private_ownership:\n        data.loc[index, 'Type of ownership'] = 'Company - Private'\n    elif ownership in non_profit:\n        data.loc[index, 'Type of ownership'] = 'Nonprofit Organization'\n    elif ownership in other_ownership:\n        data.loc[index, 'Type of ownership'] = 'Other Organization'\n        \ndata['Type of ownership'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.639206Z","iopub.execute_input":"2023-09-02T22:58:14.639587Z","iopub.status.idle":"2023-09-02T22:58:14.707022Z","shell.execute_reply.started":"2023-09-02T22:58:14.639543Z","shell.execute_reply":"2023-09-02T22:58:14.705865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 9. Industry column","metadata":{}},{"cell_type":"markdown","source":"The 'Industry' column in our dataset records the detailed operational fields of the companies, and it encompasses many unique types. Given the limited size of our dataset, I believe it would be more effective to use the 'Sector' column for our analysis, which is more general and has fewer categories compared to the 'Industry' column.","metadata":{}},{"cell_type":"code","source":"data.loc[data['Industry'] == '-1', 'Industry'] = 'Unknown'\ndata.Industry.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.708596Z","iopub.execute_input":"2023-09-02T22:58:14.708918Z","iopub.status.idle":"2023-09-02T22:58:14.720028Z","shell.execute_reply.started":"2023-09-02T22:58:14.708892Z","shell.execute_reply":"2023-09-02T22:58:14.718734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 10. Revenue column","metadata":{}},{"cell_type":"markdown","source":"For the \"Revenue\" column, a challenge arises from the data being formatted as strings, with values represented in both billions and millions. To sort these values effectively, we will create a new column to record the revenue lower bond in numerical format, and we will use 'million' as the standard unit.","metadata":{}},{"cell_type":"code","source":"# transfer -1 values to 'Unknown'\ndata.loc[data['Revenue'] == '-1', 'Revenue'] = 'Unknown / Non-Applicable'\ndata['revenue_lower_bond'] = 0\n\ndigits_in_rev = data['Revenue'].apply(lambda x: re.findall(r'\\d+', x))\n\nfor index, row in data.iterrows():\n    if 'billion' in row['Revenue'] and 'million' not in row['Revenue']:\n        data.loc[index, 'revenue_lower_bond'] = (int(digits_in_rev[index][0]) \n                                                 * 1000)\n    # the only special case\n    elif row['Revenue'] == 'Less than $1 million (USD)':\n        data.loc[index, 'revenue_lower_bond'] = 0.5\n    elif 'million' in row['Revenue']:\n        data.loc[index, 'revenue_lower_bond'] = int(digits_in_rev[index][0])\n\n# as we can see, the Revenue column now can be sorted using the \n# revenue_lower_bond column\n(data[['Revenue', 'revenue_lower_bond']]\n.loc[~data.duplicated('Revenue')].sort_values('revenue_lower_bond'))","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.721671Z","iopub.execute_input":"2023-09-02T22:58:14.722763Z","iopub.status.idle":"2023-09-02T22:58:14.858648Z","shell.execute_reply.started":"2023-09-02T22:58:14.722729Z","shell.execute_reply":"2023-09-02T22:58:14.857387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 11. Sector column","metadata":{}},{"cell_type":"code","source":"data.loc[data['Sector'] == '-1', 'Sector'] = 'Unknown'\ndata.Sector.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.860030Z","iopub.execute_input":"2023-09-02T22:58:14.861233Z","iopub.status.idle":"2023-09-02T22:58:14.872538Z","shell.execute_reply.started":"2023-09-02T22:58:14.861190Z","shell.execute_reply":"2023-09-02T22:58:14.871442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 12. Competitors column","metadata":{}},{"cell_type":"markdown","source":"Since there are way too many invalid values, the -1, in the Competitors column, we will just drop it.","metadata":{}},{"cell_type":"code","source":"print(data.Competitors.value_counts())\ndata.drop(['Competitors'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T22:58:14.874152Z","iopub.execute_input":"2023-09-02T22:58:14.874727Z","iopub.status.idle":"2023-09-02T22:58:14.889329Z","shell.execute_reply.started":"2023-09-02T22:58:14.874687Z","shell.execute_reply":"2023-09-02T22:58:14.888037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have the raw data cleaned, and the data is ready for the exploratory\ndata analysis.","metadata":{}},{"cell_type":"markdown","source":"<a id='EDA'></a>\n## **Exploratory Data Analysis**\n\nIt is always a good practice to understand the data first and try to gather as many insights from it, and Exploratory Data Analysis (EDA) is a very helpful tool for that. EDA analyzes the data and discover trends, patterns, or check assumptions in data with the help of statistical summaries and graphical representations.\n\nIn this section, I will begin by posing questions and trying to approach them by various visualizations. Then, with the help of our visualizations, we can identify the hidden patterns, relationships, and trends in the data and use them to facilitate further analysis or formulating hypotheses.","metadata":{}},{"cell_type":"markdown","source":"1.How many entries are there for each job title?","metadata":{}},{"cell_type":"code","source":"# data preparation\ntitle_count = data['Job Title'].value_counts()\n\n# plot the count of each title\nax = sns.barplot(x = title_count.index,y = title_count, color = '#5fa8d3',\n                 saturation = 1)\n\nfor bar in ax.patches:\n    ax.text(bar.get_x() + bar.get_width() / 2,\n            bar.get_height(),\n            round(bar.get_height()),\n            va = 'bottom', ha = 'center')\n\n\nax.set_title('Number of Entries for Each Job Title')\nax.set_ylabel('count')\nplt.xticks(rotation = 30, size = 8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T23:22:16.621946Z","iopub.execute_input":"2023-09-02T23:22:16.622601Z","iopub.status.idle":"2023-09-02T23:22:16.939924Z","shell.execute_reply.started":"2023-09-02T23:22:16.622540Z","shell.execute_reply":"2023-09-02T23:22:16.938568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the bar plot above, it's evident that the Data Scientist role is the most prevalent, with all other titles registering no more than 70 entries. Interestingly, senior positions are relatively rare across all titles, except for the Data Scientist role. It's worth noting that the term 'Data Scientist' is often used broadly, as the responsibilities associated with data roles can vary significantly depending on the company and project.","metadata":{}},{"cell_type":"markdown","source":"2.What is the salary distribution across different titles? Which titles have the highest/lowest salary in general?","metadata":{}},{"cell_type":"code","source":"# we can show the average salary midpoint as well as the average lower and upper \n# salary limits for each job title\nsalary_info = (data.loc[:, ['Job Title', 'salary_lower',\n                            'salary_midpoint', 'salary_upper']]\n                   .groupby('Job Title', as_index = False)\n                   .agg(np.mean)\n                   .round(1)\n                   .sort_values(by = 'salary_midpoint')\n                   .reset_index(drop = True))\n\nlong_salary_info = salary_info.melt(id_vars = ['Job Title'], \n                                    var_name = 'type',\n                                    value_name = 'value')\n\n# plot the data\nfig, ax = plt.subplots()\nfor title in salary_info['Job Title'].unique():\n    current_title_data = long_salary_info[long_salary_info['Job Title'] == title]\n    plt.plot('Job Title', 'value', data = current_title_data, lw = 10,\n             color = 'blue')\n    plt.plot(title, np.median(current_title_data['value']), \n             color = 'red', marker = '_', ms = 20, mew = 5)\nplt.xticks(rotation = 45, size = 8)\nax.set_title('average salary ranges and midpoints across titles')\nax.set_ylabel('Average Salary Midpoint')\nax.set_yticks(range(90, 170, 10))\nax.set_yticklabels(['$' + str(label) + 'K' for label in range(90, 170, 10)])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T21:32:05.291154Z","iopub.execute_input":"2023-08-23T21:32:05.291710Z","iopub.status.idle":"2023-08-23T21:32:05.648497Z","shell.execute_reply.started":"2023-08-23T21:32:05.291665Z","shell.execute_reply":"2023-08-23T21:32:05.646912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the displayed graph above, we represent the salary range for each job title, with the salary midpoint highlighted in red. And we can notice some interesting things: \n* The Data Analyst position has a notably lower salary range and midpoint compared with other general titles.\n\n* The salary midpoint for the Senior Data Engineer position is lower than that of the Data Engineer. And The salary midpoint and range for the Senior Data Scientist position is really close to that of the Data Scientist. These seem counterintuitive given the usual expectations for senior roles to command higher salaries, as what is shown between the senior data analyst and data analyst. \n     First possible reason is that the sample sizes of these senior positions are too small, as there are only 7 pieces of salary data for the senior data engineer. Secondly, Glassdoor provides salary estimates as ranges, and without insight into the distribution within these ranges, we are not able to access the true salary dynamics.\n                                   \n* The salary midpoint and range for the Data Manager position stand out as they are noticeably higher compared to the other seven titles.","metadata":{}},{"cell_type":"markdown","source":"3.How is the distribution of job positions across various regions? Are there any states that stand out with a significantly larger number of data science roles?","metadata":{}},{"cell_type":"code","source":"# Get the number of job posting in each state\nstate_counts = data.state.value_counts()\n\n# Convert the state counts to a DataFrame\nstate_counts_df = state_counts.reset_index()\nstate_counts_df.columns = ['state', 'count']\n\n# Create a choropleth map\nfig = px.choropleth(state_counts_df, \n                    locations='state', \n                    color='count', \n                    locationmode='USA-states', \n                    scope='usa', \n                    title='Number of Data Science Job Postings by State',\n                    color_continuous_scale='Blues',\n                    labels={'count':'Number of Job Postings'})\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T21:03:47.128211Z","iopub.execute_input":"2023-08-23T21:03:47.129284Z","iopub.status.idle":"2023-08-23T21:03:49.037501Z","shell.execute_reply.started":"2023-08-23T21:03:47.129251Z","shell.execute_reply":"2023-08-23T21:03:49.035439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the choropleth map above, it's evident that California stands out with a significantly higher number of job postings compared to other states. A moderate number of postings can also be seen in a few East Coast states such as Virginia, New York, and Massachusetts. In contrast, the states in the central region appear to have relatively few job postings or even no job postings.","metadata":{}},{"cell_type":"markdown","source":"4.What are the average salary midpoints in each state? Which part of the country offers a higher salary on average?","metadata":{}},{"cell_type":"code","source":"state_salary = (data.groupby('state', as_index = False)\n                [['state', 'salary_midpoint']]\n                .agg({'salary_midpoint':'mean'})).round(1)\n\nfig = px.choropleth(state_salary, locations = 'state',\n                    color = 'salary_midpoint',\n                    locationmode = 'USA-states',\n                    scope = 'usa',\n                    title = 'Average Salary Midpoint of Data Science Jobs in Each State',\n                    color_continuous_scale = 'Blues',\n                    labels={'salary_midpoint':\n                            'Average Salary Midpoint (thousand dollar)'})\nfig.show()\n\n# Delaware is an outlier which is built on a single data point\ndata[data['state'] == 'DE']","metadata":{"execution":{"iopub.status.busy":"2023-08-23T21:06:50.035077Z","iopub.execute_input":"2023-08-23T21:06:50.035539Z","iopub.status.idle":"2023-08-23T21:06:50.133123Z","shell.execute_reply.started":"2023-08-23T21:06:50.035508Z","shell.execute_reply":"2023-08-23T21:06:50.131682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the map, it's evident that most central states have comparatively lower average salary midpoints compared with most costal states. For example, New York ,North Carolina, Texas, and Washington all have an average salary midpoint over 133K. Delaware is an interesting outlier; despite being based on a single data point, it has the highest average salary midpoint at \\$271.5K.","metadata":{}},{"cell_type":"markdown","source":"5.How is company size distributed in our dataset? Are the majority large or small companies?","metadata":{}},{"cell_type":"code","source":"# create a helper function to locate the lower bound of each category\ndef size_sorter(index_values):\n    sizes = []\n    for size in index_values:\n        numbers = re.findall(r\"(\\d+\\.\\d|\\d+)\", size)\n        if len(numbers) == 0:\n            minimum = 0\n        else: \n            minimum = min(map(int, numbers))\n        sizes.append(minimum)\n    return pd.Series(sizes)\n\n\n# create the company size count df\ncompany_sizes = (data.loc[~data.duplicated('Company Name'), 'Size']\n                .value_counts().reset_index())\ncompany_sizes['lower_limit'] = size_sorter(company_sizes['index'])\ncompany_sizes = company_sizes.sort_values('lower_limit')\n\n\n# plot the bar plot\nax = sns.barplot(data = company_sizes, x = 'index', y = 'Size',\n                 color = '#5fa8d3', saturation = 1)\nfor bar in ax.patches:\n    ax.text(bar.get_x() + bar.get_width()/2,\n            bar.get_height(),\n            f'{round(bar.get_height())}',\n            ha = 'center', va = 'bottom')\nax.set_title('the number of company with each company size')\nax.set_xlabel(\"company size\")\nax.set_ylabel(\"count\")\nplt.xticks(rotation = 30, size = 7)\n# also draw a horizontal line for the average count of company size\nax.axhline(y = company_sizes.Size.mean(), color = 'red', \n           linewidth = 3, linestyle = '--')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T21:08:01.571409Z","iopub.execute_input":"2023-08-23T21:08:01.572309Z","iopub.status.idle":"2023-08-23T21:08:01.946042Z","shell.execute_reply.started":"2023-08-23T21:08:01.572270Z","shell.execute_reply":"2023-08-23T21:08:01.944845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph, we can tell although the distribution of company sizes is not well balanced, companies of all sizes have a demand for data professionals. Notably, 91 out of the 432 companies in this data set employ between 51 to 200 individuals, while a mere 25 companies in this data set have a workforce ranging from 5,001 to 10,000. No clear trend indicates a predominance of larger firms. The limited sample size might account for the absence of a distinct pattern.","metadata":{}},{"cell_type":"markdown","source":"6.How do average salary midpoints vary across different company sizes in our dataset? Which type of company offers the highest salary?","metadata":{}},{"cell_type":"code","source":"# prepare the data\ncompany_size_salary = (data.groupby('Size', as_index = False).salary_midpoint\n                      .apply(np.mean))\ncompany_size_salary['lower_bound'] = size_sorter(company_size_salary.Size)\ncompany_size_salary.sort_values('lower_bound', inplace = True)\n\n# plot bar chart\nax = sns.barplot(data = company_size_salary, x = 'Size', \n                 y = 'salary_midpoint', color = '#5fa8d3',\n                 alpha = 1, saturation = 1)\nfor bar in ax.patches:\n    ax.text(bar.get_x() + bar.get_width() / 2,\n            bar.get_height(),\n            f'{round(bar.get_height(), 1)}K',\n            ha = 'center', va = 'bottom')\nplt.xticks(rotation = 30, size = 7)\nax.set_title('average salary midpoint for different company sizes')\nax.set_xlabel('company size')\nax.set_ylim([0, 160])\n# denote the overall average salary midpoint using the red line\nax.axhline(y = company_size_salary.salary_midpoint.mean(), color = 'red',\n           linestyle = '-')\nax.set_ylabel('Average Salary Midpoint')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T21:26:31.117425Z","iopub.execute_input":"2023-08-23T21:26:31.117914Z","iopub.status.idle":"2023-08-23T21:26:31.459798Z","shell.execute_reply.started":"2023-08-23T21:26:31.117880Z","shell.execute_reply":"2023-08-23T21:26:31.458204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph presented, there is no substantial variation in the average salary midpoints across different company sizes; the figures closely align with the overall average salary midpoint. Consequently, the data does not provide strong evidence to support the hypothesis that larger companies typically offer higher salaries.","metadata":{}},{"cell_type":"markdown","source":"7.How many job postings are there in each business sector? Which one on average offer the highest salary?","metadata":{}},{"cell_type":"code","source":"# prepare the data \nsector_salary = (data.groupby('Sector', as_index = False)\n                     .agg({'salary_midpoint':['mean', 'size']}).round(1))\n\nsector_salary.columns = ['sector', 'salary_midpoint', 'number_of_data']\nsector_salary = (sector_salary.query('(number_of_data > 10) & (sector != \"Unknown\")')\n                             .sort_values('number_of_data', ascending = False))\n\n\n# plot the number of data in each business sector\nplt.figure(1)\nax1 = sns.barplot(data = sector_salary, x = 'sector', y = 'number_of_data',\n                  color = '#5fa8d3', alpha = 1, saturation = 1)\nfor bar in ax1.patches:\n    ax1.text(bar.get_x() + bar.get_width() / 2,\n            bar.get_height(),\n            f'{round(bar.get_height())}',\n            va = 'bottom', ha = 'center', size = 8.5)\nax1.set_title('The Number of Job Posting across Business Sectors')\nax1.set_ylabel('count')\nplt.xticks(rotation = 30, size = 7)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T21:17:54.479872Z","iopub.execute_input":"2023-08-23T21:17:54.480318Z","iopub.status.idle":"2023-08-23T21:17:54.887735Z","shell.execute_reply.started":"2023-08-23T21:17:54.480289Z","shell.execute_reply":"2023-08-23T21:17:54.886279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph illustrates that the Information and Technology sector is at the forefront, offering a substantial number of job opportunities, followed by the Business Services and Biotech & Pharmaceuticals sectors. Other sectors are shown to have a moderate level of data science job postings, with none exceeding 50 positions.","metadata":{}},{"cell_type":"code","source":"# plot the salary midpoint of each sector\nsector_salary = sector_salary.sort_values('salary_midpoint')\nplt.figure(2)\nax2 = sns.barplot(data = sector_salary, x = 'sector', y = 'salary_midpoint',\n            color = '#5fa8d3', alpha = 1, saturation = 1)\n\nfor bar in ax2.patches:\n    ax2.text(bar.get_x() + bar.get_width() / 2,\n            bar.get_height(),\n            f'{bar.get_height()}K',\n            va = 'bottom', ha = 'center', size = 8.5)\nax2.axhline(y = data.salary_midpoint.mean(), color = 'red')\nplt.xticks(rotation = 30, size = 7)\nax2.set_ylim([0,150])\nax2.set_yticks(range(0, 160, 10))\nax2.set_title('The Avergae Salary Midpoint across Business Sectors')\nax2.set_ylabel('Average Salary Midpoint')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T21:26:03.726114Z","iopub.execute_input":"2023-08-23T21:26:03.726731Z","iopub.status.idle":"2023-08-23T21:26:04.258165Z","shell.execute_reply.started":"2023-08-23T21:26:03.726689Z","shell.execute_reply":"2023-08-23T21:26:04.256878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nIn the second bar plot, the average salary midpoints are displayed exclusively for sectors with more than 10 data entries. Notably, the Government and Aerospace & Defense sectors emerge as offering the highest salary midpoints, while the Finance and Insurance sectors appear to lag, falling on the lower end of the spectrum. The difference between these extremes is \\$23.2K, which, considering the range of salaries in the field, is not a substantial gap.","metadata":{}},{"cell_type":"markdown","source":"8.What are the average salary midpoints in different types of business ownership?","metadata":{}},{"cell_type":"code","source":"salary_onwership = (data.groupby('Type of ownership', as_index = False)\n                    [['Type of ownership', 'salary_midpoint']]\n                    .agg({'salary_midpoint':'mean'})\n                    .round(1).sort_values('salary_midpoint'))\n\n# plot the graph\nax = sns.barplot(data = salary_onwership, x = 'Type of ownership',\n                 y = 'salary_midpoint', color = '#5fa8d3', \n                 alpha = 1, saturation = 1)\n\nfor bar in ax.patches:\n    ax.text(bar.get_x() + bar.get_width() / 2,\n            bar.get_height(),\n            f'{bar.get_height()}K',\n            va = 'bottom', ha = 'center')\nplt.xticks(rotation = 20, size = 7)\nax.set_title('average salary midpoints across types of business ownership')\nax.set_ylim([0, 150])\nax.set_ylabel('Average Salary Midpoint')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T21:26:11.288909Z","iopub.execute_input":"2023-08-23T21:26:11.289440Z","iopub.status.idle":"2023-08-23T21:26:11.588622Z","shell.execute_reply.started":"2023-08-23T21:26:11.289400Z","shell.execute_reply":"2023-08-23T21:26:11.587170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We didn't observe a substantial difference in average salary midpoints across various ownership types.","metadata":{}},{"cell_type":"markdown","source":"9.Do companies with greater revenues tend to offer higher average salary midpoints?","metadata":{}},{"cell_type":"code","source":"salary_revenue = (data.groupby('Revenue', as_index = False)\n                    [['Revenue', 'salary_midpoint', 'revenue_lower_bond']]\n                    .agg({'salary_midpoint':'mean', \n                          'revenue_lower_bond':'mean'})\n                    .round(1).sort_values('revenue_lower_bond'))\n\n#plot\nax = sns.barplot(data = salary_revenue, y = 'Revenue',\n                 x = 'salary_midpoint', color = '#5fa8d3', \n                 alpha = 1, saturation = 1)\nfor bar in ax.patches:\n    ax.text(bar.get_x() + bar.get_width(),\n            bar.get_y() + bar.get_height() / 2,\n            f'{bar.get_width()}K',\n            va = 'center', ha = 'right', size = 7)\nax.axvline(x = data['salary_midpoint'].mean(), color = 'red', lw = 1)\nax.set_title('Average Salary Midpoints across Various Revenue Levels')\nax.set_ylabel('Revenue Level')\nax.set_xlabel('Average Salary Midpoint')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:29:21.827562Z","iopub.execute_input":"2023-08-22T16:29:21.828191Z","iopub.status.idle":"2023-08-22T16:29:22.495806Z","shell.execute_reply.started":"2023-08-22T16:29:21.828163Z","shell.execute_reply":"2023-08-22T16:29:22.495097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the horizontal bar plot above, we compare average salary midpoints across different company revenue levels. While most revenue categories closely align with the overall salary midpointrepresented by the red vertical linefour categories deviate noticeably. The categories of 5 to 10 million, 500 million to 1 billion, and 5 to 10 billion all have average salary midpoints exceeding \\$130K. Interestingly, the 10+ billion category stands out as the sole group with revenues exceeding 1 billion that registers below the overall average salary midpoint. Conversely, the 5 to 10 million category is the only group with revenues under 1 billion but with a salary midpoint higher than average. This pattern to some extent suggests that the companies with a higher revenue tend to provide a higher salary midpoint.\n\nAnother thing I have to mention is that the average salary midpoint for the 50 to 100 million revenue category is only \\$97.9K, notably lower than other categories. This led me to question the representativeness of the data for this category. Upon closer examination, I found that the 50 to 100 million category comprises 32 data entries. Evaluating its diversity across various dimensions, the data is robust in key areas: it encompasses 6 of the 8 unique titles, represents 23 distinct companies across 10 different sectors, and spans 14 states. \n\nHowever, despite these diverse data points, there's a level of uncertainty  due to the limited sample size, and we should be wary of generalizing this finding without additional data or external validation.","metadata":{}},{"cell_type":"code","source":"# extract the number of unique value in each column of the data of\n# the 50-100million group\ndiversity = (data.loc[data['revenue_lower_bond'] == 50]\n.agg([lambda col: data[col.name].unique().shape[0],\n      lambda col: col.unique().shape[0]])\n.transpose())\ndiversity.columns = ['total number of unique values in dataset', \n                     'number of unique values in 50-100 million group']\ndiversity","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:29:22.496763Z","iopub.execute_input":"2023-08-22T16:29:22.497083Z","iopub.status.idle":"2023-08-22T16:29:22.547635Z","shell.execute_reply.started":"2023-08-22T16:29:22.497054Z","shell.execute_reply":"2023-08-22T16:29:22.546591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='conclusion'></a>\n## **Conclusion**","metadata":{}},{"cell_type":"markdown","source":"1. Based on our dataset, the 'Data Analyst' role has the lowest average salary midpoint, while the 'Data Science Manager' role commands the highest average salary midpoint. The 'Data Engineer,' 'Data Scientist,' and 'Machine Learning Scientist' roles exhibit relatively similar salary midpoints and ranges. As for the senior positions, we were unable to find clear evidence to support our hypothesis that senior titles, on average, command higher salaries.\n\n2. The choropleth maps in our analysis reveal the significant geographical variations in the data science job market across the United States, with coastal states generally offering more opportunities and higher salaries compared to central states. California stands as a significant hotspot, boasting a high number of job postings, although its average salary midpoints are not exceptionally high when compared to certain other states. The East Coast, particularly New York, Virginia, and Massachusetts, also emerges as a key region with a substantial number of job postings and competitive salary midpoints.\n\n3. The analysis also shows interesting insights at the company level. The Information and Technology sector stands out as a leader in job opportunities, indicating a robust demand for data professionals in technology-centric industries. However, the data suggsted that a high demand for data science roles in certain sectors does not necessarily correlate with higher salaries.\n\n4. While the data doesn't strongly support the hypothesis that larger companies invariably offer higher salaries, it does indicate that companies with a revenue exceeding 1 billion dollars tend to have higher salary midpoints.","metadata":{}},{"cell_type":"markdown","source":"<a id='discussion'></a>\n## **Discussion**","metadata":{}},{"cell_type":"markdown","source":"#### 1. Analysis Implications\nThe insights from this analysis can serve as a valuable guide for data science job seekers as they craft their career strategies. \n\n* Firstly, while the Information and Technology sector is a hotbed for data science opportunities, it is advisable for professionals to explore alternative sectors as well. Some sectors, such as Government and Aerospace & Defense, may have fewer positions available, but they tend to offer higher compensation. This scenario underscores the importance of cultivating specialized skills that align with the unique demands of these sectors.\n\n* Secondly, location matters; coastal states like California and New York not only offer a high number of job opportunities but also competitive salaries. However, individuals must weigh these prospects against the cost of living in these regions. \n\n* Lastly, the data does not support the common assumption that larger companies invariably offer higher salaries. As such, job seekers should cast a wider net, considering positions in smaller or medium-sized companies, which might offer competitive salaries, potentially lower stress levels, and a more intimate work environment.\n\n<br>\n<br>\n\n#### 2. Limitations:\nWhile these insights offer valuable guidance for data science job seekers, it is important to recognize that this analysis has its limitations. The following are some of the limitations of this study that should be considered when interpreting its findings:\n* Outdated Data: The dataset was recorded in 2021, and as the analysis is being conducted in 2023, there might have been significant changes in the data science job market. The two-year gap could have resulted in shifts in salary ranges, job demand in various locations, and industry hiring trends. This time lag may affect the current relevance and applicability of the findings.\n\n* Limited Sample Size: The dataset contains only 672 job postings, which may not be sufficiently large to accurately represent the entire data science job market. For example, there is only one data entry for Delaware, and only 3 job postings for the Education sector. This limited sample size could potentially skew the analysis and may not capture the full scope of the job market.\n\n* Source Bias and Data Integrity: The data was web scraped from Glassdoor. Relying on a single source for data can introduce bias, as Glassdoors listings may not be fully representative of the broader job market. Meanwhile, because we could not validate the integrity of the scraped data, this raises questions about the accuracy and completeness of the information used in this analysis.\n\n* Salary Estimates: The dataset does not provide fixed salary values, but rather a wide range estimated by Glassdoor. Additionally, the analysis is based on salary midpoints, which are calculated as the middle value between the given upper and lower limits. Since the actual distribution of salaries within this range is unclear, this approach may not accurately reflect the actual salary offers. This could, in turn, affect the reliability of the salary insights generated by the analysis.\n\n* Title Consolidation: In the analysis, we consolidated the job titles into 8 general categories. However, the responsibilities of a data science professional can vary significantly based on their main field. This title aggregation process may introduce potential bias.","metadata":{}},{"cell_type":"markdown","source":"<a id='acknowledgement'></a>\n## **Acknowledgement**\n1. I would like to extend my gratitude to Rashik Rahman for providing the data used in this analysis. The dataset has been instrumental in allowing this comprehensive exploration of the data science job market. Data Source Link: https://www.kaggle.com/datasets/rashikrahmanpritom/data-science-job-posting-on-glassdoor","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}